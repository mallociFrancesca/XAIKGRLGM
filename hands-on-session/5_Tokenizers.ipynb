{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_a9QvUFVCUR"
   },
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01ti95qsClaB"
   },
   "source": [
    "In this notebook, you'll learn how to work with tokenizers using the Hugging Face library.\n",
    "\n",
    "We'll explore several tokenizers from different Large Language Models (LLMs) and see how each one handles tokenization differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ6sZdX5ClaB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## \u2699\ufe0f Setup Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1vi59IYClaC"
   },
   "source": [
    "We start with setting up the workspace by installing the `transformers` library and ignoring the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31685,
     "status": "ok",
     "timestamp": 1751474983745,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "AbbeCGc0CodM",
    "outputId": "ec15d1f0-b7d7-4b22-fd37-6f598559b8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30,
    "id": "TEv_ive_ClaC"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64,
    "id": "VRBsRcgIClaC"
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADatC0QUClaD"
   },
   "source": [
    "## \ud83e\udde9 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ytGOnnXClaD"
   },
   "source": [
    "### Tokenization Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAafoqs7ClaD"
   },
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>\ud83d\udca1 </b> <b>Tokenizers</b> are essential tools in Natural Language Processing (NLP), acting as a bridge between human language and machine learning models. </br>\n",
    "Their main role is to <b>break down raw text into tokens</b> \u2014units like words, subwords, or characters\u2014and then convert these tokens into  <b>numerical IDs</b> that models can understand.</p><br>\n",
    "This process, called <b>tokenization</b>, is a key step before feeding text into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "executionInfo": {
     "elapsed": 1810,
     "status": "ok",
     "timestamp": 1751475004747,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "PllHCM_UC049",
    "outputId": "1a1bb6f0-7565-4577-dc80-bb721ca8ae6e"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/tokenization.png\" alt=\"Tokenization Diagram\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSPYbXUFClaD"
   },
   "source": [
    "### Tokenization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_88crmReClaE"
   },
   "source": [
    "An implementation of a tokenizer consists of the following pipeline of processes, each applying different transformations to the textual information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1751475026965,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "6i7vonAqC8BE",
    "outputId": "f681f86d-4ac3-4c3c-ff53-e81ac9eecaa0"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/tokenization-pipeline.png\" alt=\"Tokenization Pipeline\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsjnFrHfClaE"
   },
   "source": [
    "- **Normalization**: Standardize text (remove accents, lowercase...)\n",
    "- **Pre-tokenization**: Split text into basic units\n",
    "- **Model**: Apply model-specific token rules\n",
    "- **Postprocessor**: Add special tokens ([CLS], [SEP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtXUNi69ClaE"
   },
   "source": [
    "There are various tokenization methods, and typically, a tokenizer must be trained on a dataset.\n",
    "\n",
    "Generally, each model has its own tokenizer.\n",
    "\n",
    "However, the Hugging Face library provides pre-trained tokenizers that can be used out of the box \ud83e\udd17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG26DfQ_ClaE"
   },
   "source": [
    "<p style=\"background-color:#f2f2ff; padding:15px; border-width:3px; border-color:#e2e2ff; border-style:solid; border-radius:6px\">\n",
    "\ud83e\ude84 The transformers library has a set of <b>Auto classes</b>, like <code>AutoConfig</code>,  <code>AutoModel</code>,and <code>AutoTokenizer</code>. <br> The <b>Auto classes</b> are designed to automatically do the job for you.</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEYSFUHeClaE"
   },
   "source": [
    "Let\u2019s see how to tokenize a text using the `AutoTokenizer` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFhmcEzCClaE"
   },
   "source": [
    "## \u2702\ufe0f Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNhnNQ8gClaE"
   },
   "source": [
    "In this section, you will tokenize the sentence **Hello World!** using the tokenizer of the [`Helsinki-NLP/opus-mt-en-it` translation model](https://huggingface.co/Helsinki-NLP/opus-mt-en-it).\n",
    "\n",
    "Let's import the `Autotokenizer` class, define the sentence to tokenize, and instantiate the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30,
    "id": "WTdHa-YBClaE",
    "outputId": "4f03e941-f2e9-430d-9789-89cd3c37546f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.marian.tokenization_marian.MarianTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Specify the model name\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-it'\n",
    "\n",
    "# load the pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47,
    "id": "00U86gpwClaE",
    "outputId": "2e20d925-1db7-45d9-f1ec-213e49e06468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [226, 1127, 499, 49, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# define the sentence to tokenize\n",
    "sentence = \"Hello world!\"\n",
    "# Apply the tokenizer to the sentence.\n",
    "tokens = tokenizer(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONeQnlUBClaF"
   },
   "source": [
    "The tokeziner splits the sentence into tokens and returns the IDs of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZshIlptClaF"
   },
   "source": [
    "The tokenizer returns a dictionary containing:\n",
    "\n",
    "* [input_ids](https://huggingface.co/docs/transformers/main/en/./glossary#input-ids): numerical representations of the tokens.\n",
    "* [attention_mask](https://huggingface.co/docs/transformers/main/en/.glossary#attention-mask): indicates which tokens should be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47,
    "id": "jU9YN8oAClaF",
    "outputId": "15af950f-3548-4c3d-9e20-86349c111617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[226, 1127, 499, 49, 0]\n"
     ]
    }
   ],
   "source": [
    "# If we want directly extract the token ids\n",
    "token_ids = tokenizer(sentence).input_ids\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4W5Ize3pClaF"
   },
   "source": [
    "To map each token ID to its corresponding token, you can use the `decode` method of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47,
    "id": "csN-QvMqClaF",
    "outputId": "9a7eb283-1485-4f68-d1e1-610a4c16d76d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "ello\n",
      "world\n",
      "!\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "for id in token_ids:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh6J4D8nClaF"
   },
   "source": [
    "The special token `</s>` is automatically added by the tokenizer and it is used to indicate the end of the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FU633SEClaF"
   },
   "source": [
    "## \ud83d\udd21 Understanding Tokenization Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWWHmOTuClaF"
   },
   "source": [
    "Each token the model understands is mapped to a unique integer ID.\n",
    "\n",
    "To better understand how this mapping works, we can access the tokenizer vocabulary via the `.get_vocab()` method, which provides the mapping between tokens and respective IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrvJ6EHpClaF"
   },
   "source": [
    "**Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Knqdz_xsClaF",
    "outputId": "37984db1-d9fe-4519-eb7f-b34cb9305eb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.2.2007,': 53163,\n",
       " '\u0e19\u0e35': 57478,\n",
       " '\uac00': 56056,\n",
       " '\u2581probation': 38013,\n",
       " '\u767d\u8272\u8774\u8776\u5170': 77080,\n",
       " '\u2581Ringrazi': 16562,\n",
       " '\u2581Coreper': 36359,\n",
       " '\u2581Chloro': 50328,\n",
       " '\u25816,': 5763,\n",
       " '42/2010': 51362}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Get the tokenizer's vocabulary (token -> ID)\n",
    "vocabulary = tokenizer.get_vocab()\n",
    "\n",
    "# Get all the tokens (i.e., the vocabulary keys)\n",
    "vocab_keys = list(vocabulary.keys())\n",
    "\n",
    "# Shuffle the tokens randomly\n",
    "random.shuffle(vocab_keys)\n",
    "\n",
    "# Show 10 random tokens along with their IDs\n",
    "{ k: vocabulary[k] for k in vocab_keys[:10] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wzlSHsKClaF",
    "outputId": "7b876375-8e05-4ddc-a3f4-ffadb03d9379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 80035\n"
     ]
    }
   ],
   "source": [
    "print(\"Total vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcmgIX_6ClaF"
   },
   "source": [
    "The `vocabulary size` indicates that the tokenizer can recognize and encode 80,035 distinct token types.\n",
    "\n",
    "What are the **special tokens** in this vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fei7swB9ClaG",
    "outputId": "16908dda-0da0-4d24-c9c8-1ff73f8e2fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of special tokens: 3\n",
      "Special tokens: ['</s>', '<unk>', '<pad>']\n",
      "Special tokens ID: [0, 1, 80034]\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all special tokens\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "\n",
    "# Get the corresponding IDs of the special tokens\n",
    "special_token_ids = tokenizer.all_special_ids\n",
    "\n",
    "print(f\"Number of special tokens: {len(special_tokens)}\")\n",
    "print(f\"Special tokens: {special_tokens}\")\n",
    "print(f\"Special tokens ID: {special_token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90fyZYwPClaG"
   },
   "source": [
    "* `</s>`: special token used to mark the end-of-the-sentence\n",
    "* `<unk>`: unknown token\n",
    "* `<pad>`: used to pad shorter sequences in a batch so that all inputs are the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX5m23Z1ClaG"
   },
   "source": [
    "Indeed, note that the tokens have a 0 showing up at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wq911ZsoClaG",
    "outputId": "ba6bc06d-f80f-44fb-d78b-3e67c5bc5acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!</s>\n",
      "[226, 1127, 499, 49, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZXoE95VClaG"
   },
   "source": [
    "We can include special tokens inside of the strings themselves. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb9kZ1XaClaJ",
    "outputId": "15772409-28bc-4fc2-c8c6-43fbf09b3391"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1711, 1127, 49, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello!</s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKTfY75KClaJ"
   },
   "source": [
    "The tokenizer does not treat it as a normal word, but interprets it as a special token already present.\n",
    "\n",
    "So it will not be \u201ctokenized\u201d into multiple pieces, but treated as a unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojq9ciTfClaJ"
   },
   "source": [
    "> \u26a0\ufe0f These details are specific to the tokenizer being used.\n",
    "\n",
    ">  **The tokenizer is responsible for deciding how to tokenize the input text.**\n",
    "\n",
    "> As a result, you might notice varying behavior depending on which tokenizer is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmqWAmDwClaJ"
   },
   "source": [
    "Now, in the next session, let\u2019s examine different behaviors by comparing three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPsVcut5ClaJ"
   },
   "source": [
    "## \ud83d\udd0d Visualizing Tokenization Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbBlBRVfClaJ"
   },
   "source": [
    "In this section, you compare the tokenizer of   `Helsinki-NLP/opus-mt-en-it ` with those of `Llama-2-7b-mt-Italian-to-English` and `bert-base-cased`, focusing on their respective tokenization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 115,
    "id": "XDEF1pi9ClaK"
   },
   "outputs": [],
   "source": [
    "# Define the text to use for comparing different tokenization strategies of each model\n",
    "text = \"\"\"\n",
    "Language Mix & punctuation!\n",
    "\ud83e\udde0 HOLA\n",
    "display_tokens Pizza True if != <= import: one tab:\"\\t\" Four tabs: \"        \"\n",
    "7.5+42=49.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtIexIYsClaK"
   },
   "source": [
    "You'll wrap the code from the previous section into a function called `show_tokens`.\n",
    "The function takes in a text and the model name, and prints the vocabulary length of the tokenizer and a colored list of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 421,
    "id": "7W0xFIVo5A0S"
   },
   "outputs": [],
   "source": [
    "# A list of colors in RGB for representing the tokens\n",
    "colors = [\n",
    "    '102;194;165', '252;141;98', '141;160;203',\n",
    "    '231;138;195', '166;216;84', '255;217;47'\n",
    "]\n",
    "\n",
    "def show_tokens(sentence: str, tokenizer_name: str):\n",
    "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
    "\n",
    "    # Load the tokenizer and tokenize the input\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "\n",
    "    # Extract vocabulary length\n",
    "    print(f\"Vocab length: {len(tokenizer)}\")\n",
    "\n",
    "    # Print a colored list of tokens\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
    "            tokenizer.decode(t) +\n",
    "            '\\x1b[0m',\n",
    "            end=' '\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBmxDyXAClaK"
   },
   "source": [
    "Consider the following features when you're doing your comparison:\n",
    "- Vocabulary length\n",
    "- Special tokens\n",
    "- Tokenization of the tabs, special characters and special keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGbCdgOXClaK"
   },
   "source": [
    "<code>\n",
    "text = \"\"\"\n",
    "Language Mix & punctuation!\n",
    "\ud83e\udde0 HOLA\n",
    "display_tokens Pizza True if != <= import: one tab:\"\\t\" Four tabs: \"        \"\n",
    "7.5+42=49.5\n",
    "\"\"\"\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxIiAHheClaK"
   },
   "source": [
    "**Helsinki-NLP/opus-mt-en-it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOHJrRQVClaK",
    "outputId": "0ba253ab-b196-4eda-ba71-87c8904cb850",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 80035\n",
      "\u001b[0;30;48;2;102;194;165mLanguage\u001b[0m \u001b[0;30;48;2;252;141;98mMix\u001b[0m \u001b[0;30;48;2;141;160;203m&\u001b[0m \u001b[0;30;48;2;231;138;195mp\u001b[0m \u001b[0;30;48;2;166;216;84mun\u001b[0m \u001b[0;30;48;2;255;217;47mctu\u001b[0m \u001b[0;30;48;2;102;194;165mation\u001b[0m \u001b[0;30;48;2;252;141;98m!\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m<unk>\u001b[0m \u001b[0;30;48;2;166;216;84mH\u001b[0m \u001b[0;30;48;2;255;217;47mOLA\u001b[0m \u001b[0;30;48;2;102;194;165mdisplay\u001b[0m \u001b[0;30;48;2;252;141;98m_\u001b[0m \u001b[0;30;48;2;141;160;203mto\u001b[0m \u001b[0;30;48;2;231;138;195mken\u001b[0m \u001b[0;30;48;2;166;216;84ms\u001b[0m \u001b[0;30;48;2;255;217;47mP\u001b[0m \u001b[0;30;48;2;102;194;165mizza\u001b[0m \u001b[0;30;48;2;252;141;98mTrue\u001b[0m \u001b[0;30;48;2;141;160;203mif\u001b[0m \u001b[0;30;48;2;231;138;195m!\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m<=\u001b[0m \u001b[0;30;48;2;102;194;165mimport\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203mone\u001b[0m \u001b[0;30;48;2;231;138;195mtab\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98mFour\u001b[0m \u001b[0;30;48;2;141;160;203mtab\u001b[0m \u001b[0;30;48;2;231;138;195ms\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m7.5\u001b[0m \u001b[0;30;48;2;141;160;203m+\u001b[0m \u001b[0;30;48;2;231;138;195m42\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m49\u001b[0m \u001b[0;30;48;2;102;194;165m.5\u001b[0m \u001b[0;30;48;2;252;141;98m</s>\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"Helsinki-NLP/opus-mt-en-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZACmUAOClaK"
   },
   "source": [
    "**Llama-2-7b-mt-Italian-to-English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_097ETrjClaK",
    "outputId": "df54adc5-3b36-4c4a-8c73-96fc9ae584b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 32000\n",
      "\u001b[0;30;48;2;102;194;165m<s>\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
      "\u001b[0m \u001b[0;30;48;2;231;138;195mLanguage\u001b[0m \u001b[0;30;48;2;166;216;84mMix\u001b[0m \u001b[0;30;48;2;255;217;47m&\u001b[0m \u001b[0;30;48;2;102;194;165mpun\u001b[0m \u001b[0;30;48;2;252;141;98mctu\u001b[0m \u001b[0;30;48;2;141;160;203mation\u001b[0m \u001b[0;30;48;2;231;138;195m!\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
      "\u001b[0m \u001b[0;30;48;2;255;217;47m\ufffd\u001b[0m \u001b[0;30;48;2;102;194;165m\ufffd\u001b[0m \u001b[0;30;48;2;252;141;98m\ufffd\u001b[0m \u001b[0;30;48;2;141;160;203m\ufffd\u001b[0m \u001b[0;30;48;2;231;138;195mHO\u001b[0m \u001b[0;30;48;2;166;216;84mLA\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
      "\u001b[0m \u001b[0;30;48;2;102;194;165mdisplay\u001b[0m \u001b[0;30;48;2;252;141;98m_\u001b[0m \u001b[0;30;48;2;141;160;203mto\u001b[0m \u001b[0;30;48;2;231;138;195mkens\u001b[0m \u001b[0;30;48;2;166;216;84mP\u001b[0m \u001b[0;30;48;2;255;217;47mizza\u001b[0m \u001b[0;30;48;2;102;194;165mTrue\u001b[0m \u001b[0;30;48;2;252;141;98mif\u001b[0m \u001b[0;30;48;2;141;160;203m!=\u001b[0m \u001b[0;30;48;2;231;138;195m<=\u001b[0m \u001b[0;30;48;2;166;216;84mimport\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165mone\u001b[0m \u001b[0;30;48;2;252;141;98mtab\u001b[0m \u001b[0;30;48;2;141;160;203m:\"\u001b[0m \u001b[0;30;48;2;231;138;195m\t\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47mFour\u001b[0m \u001b[0;30;48;2;102;194;165mtabs\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m      \u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
      "\u001b[0m \u001b[0;30;48;2;102;194;165m7\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m5\u001b[0m \u001b[0;30;48;2;231;138;195m+\u001b[0m \u001b[0;30;48;2;166;216;84m4\u001b[0m \u001b[0;30;48;2;255;217;47m2\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m4\u001b[0m \u001b[0;30;48;2;141;160;203m9\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m5\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"kaitchup/Llama-2-7b-mt-Italian-to-English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92SwMi66ClaK"
   },
   "source": [
    "**bert-base-cased**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1725544666773,
     "user": {
      "displayName": "Jay Alammar \u062c\u0647\u0627\u062f \u0627\u0644\u0639\u0645\u0627\u0631",
      "userId": "14617748739431919458"
     },
     "user_tz": 240
    },
    "height": 30,
    "id": "fCDGSXP75Hv-",
    "outputId": "f2c26835-a857-41db-ff2d-d930d06e512e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 28996\n",
      "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mLanguage\u001b[0m \u001b[0;30;48;2;141;160;203mMix\u001b[0m \u001b[0;30;48;2;231;138;195m&\u001b[0m \u001b[0;30;48;2;166;216;84mpu\u001b[0m \u001b[0;30;48;2;255;217;47m##nc\u001b[0m \u001b[0;30;48;2;102;194;165m##tu\u001b[0m \u001b[0;30;48;2;252;141;98m##ation\u001b[0m \u001b[0;30;48;2;141;160;203m!\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mH\u001b[0m \u001b[0;30;48;2;255;217;47m##OL\u001b[0m \u001b[0;30;48;2;102;194;165m##A\u001b[0m \u001b[0;30;48;2;252;141;98mdisplay\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtoken\u001b[0m \u001b[0;30;48;2;166;216;84m##s\u001b[0m \u001b[0;30;48;2;255;217;47mPizza\u001b[0m \u001b[0;30;48;2;102;194;165mTrue\u001b[0m \u001b[0;30;48;2;252;141;98mif\u001b[0m \u001b[0;30;48;2;141;160;203m!\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m<\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165mimport\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203mone\u001b[0m \u001b[0;30;48;2;231;138;195mta\u001b[0m \u001b[0;30;48;2;166;216;84m##b\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203mFour\u001b[0m \u001b[0;30;48;2;231;138;195mta\u001b[0m \u001b[0;30;48;2;166;216;84m##bs\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m7\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m5\u001b[0m \u001b[0;30;48;2;255;217;47m+\u001b[0m \u001b[0;30;48;2;102;194;165m42\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m49\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m5\u001b[0m \u001b[0;30;48;2;255;217;47m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25BwWZhrClaK",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "**\ud83d\udc49 Note:**\n",
    "Each model typically defines its own set of special tokens.\n",
    "Some of these tokens are essential during training, while others may be helpful during inference.\n",
    "\n",
    "The `tokenizer` object provides convenient attributes to access these special tokens. Here are some common examples:\n",
    "\n",
    "- **`pad_token`** \u2014 used to pad input sequences to the same length (as discussed later),\n",
    "- **`bos_token`** and **`eos_token`** \u2014 mark the beginning and end of a sequence, respectively,\n",
    "- **`mask_token`** \u2014 used to mask tokens during training,\n",
    "- **`sep_token`** \u2014 separates segments or sentences (e.g., next sentence prediction),\n",
    "- **`cls_token`** \u2014 indicates the start of a sequence (e.g., for classification tasks),\n",
    "- **`unk_token`** \u2014 used to indicate out-of-vocabulary tokens (i.e. tokens that are not in the vocabulary).\n",
    "\n",
    "> \u26a0\ufe0f  Not all tokenizers use all of these special tokens.\n",
    "If a token isn't used by a specific tokenizer, its corresponding attribute will be set to `None`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7KkBv_yClaK"
   },
   "source": [
    "Now, let\u2019s finish this notebook by examining how the tokenizer works when it has to process multiple sentences at once, and how padding tokens are used in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHVCxSynClaK"
   },
   "source": [
    "## \ud83d\udcda Batch Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8QIsn-9ClaL"
   },
   "source": [
    "Generally, esplecially at the training time, we want to process several text sequence at once (e.g., an entire batch of sentences).\n",
    "\n",
    "A tokenizer can also accept a list of sentences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fiv4u-_pClaL",
    "outputId": "626999a5-c3a2-4354-aa6f-8a72bfe30fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[888, 308, 11347, 0]\n",
      "[22, 5, 98, 4, 993, 40, 11347, 0]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"My first sentence\",\n",
    "    \"I'm the seconde sentence\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sentences)\n",
    "\n",
    "for id in tokens[\"input_ids\"]:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoKsFUrfClaL"
   },
   "source": [
    "> \u26a0\ufe0f Sentences of different lengths contain a different number of tokens.\n",
    "However, for the model to process them properly, the input tensors must have **uniform dimensions** \u2014 meaning the same length across all sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yzd2m2YClaL"
   },
   "source": [
    "<p style=\"background-color:#f2f2ff; padding:15px; border-width:3px; border-color:#e2e2ff; border-style:solid; border-radius:6px\">\n",
    "To achieve this, we use <b>padding</b>: shorter sentences are extended with special padding tokens (such as <code>PAD</code>) so they match the length of the longest sentence in the batch.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0gDVPFXClaL"
   },
   "source": [
    "Since these padding tokens aren't part of the actual input content, it's important to tell the model to ignore them during processing.\n",
    ">\n",
    "> That's where the **`attention_mask`** comes in \u2014 it tells the model which tokens are real and which are just padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvGfUEQqClaL",
    "outputId": "7b7db87b-ba94-46dc-c097-c7fffbe4e0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[888, 308, 11347, 0, 80034, 80034, 80034, 80034] [1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[22, 5, 98, 4, 993, 40, 11347, 0] [1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Use the tokenizer to tokenize the sentences with padding and print the result (use padding=True)\n",
    "tokens = tokenizer(sentences, padding=True)\n",
    "\n",
    "for tok, att in zip(tokens[\"input_ids\"], tokens[\"attention_mask\"]):\n",
    "    print(tok, att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFYGkD7sClaL"
   },
   "source": [
    "To match the length of the second sentence, the first one is padded with `80034`s \u2014 these represent the `<pad>` token (whose ID is `80034`).\n",
    "\n",
    "Similarly, the `attention_mask` for the first sentence includes `0`s in the positions where padding was added. This signals the model to skip those tokens during processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFS-LEJZClaL"
   },
   "source": [
    "For completeness, we can also decode batches of sentences, with `tokenizer.batch_decode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aE8XZKJ5ClaL",
    "outputId": "04292156-6831-4db5-f7c9-2ccff75c06c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My first sentence</s> <pad> <pad> <pad> <pad>',\n",
       " \"I'm the seconde sentence</s>\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokens[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-6jjxvAClaL"
   },
   "source": [
    "# References\n",
    "<a name=\"r1\">[1]</a> Alammar, J., & Grootendorst, M. (2024). Hands-On Large Language Models. O'Reilly Media. ISBN 9781098150969\n",
    "\n",
    "<a name=\"r2\">[2]</a> https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "\n",
    "<a name=\"r3\">[3]</a> https://medium.com/@awaldeep/hugging-face-understanding-tokenizers-1b7e4afdb154\n",
    "\n",
    "<a name=\"r4\">[4]</a> DataBase and Data Mining Group Research Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hIaowTQClaL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}