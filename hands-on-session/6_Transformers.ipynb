{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yH8UL6lqDNoA"
   },
   "source": [
    "# Tranformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roSWKfKfDNoB"
   },
   "source": [
    "In this notebook, you'll learn how to use the Hugging Face Transformers library for the translation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IYwL-z3DNoB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### \u2699\ufe0f Setup Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q5AzzyxDNoC"
   },
   "source": [
    "We start with setting up the workspace by installing the `transformers` library and ignoring the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78994,
     "status": "ok",
     "timestamp": 1751475187742,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "CppISwFaDOyq",
    "outputId": "1c2755aa-4c3b-4cf7-adb6-9816bd61d85f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15YfS-MhDNoC"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "# required by the Helsinki-NLP/opus-mt-en-it model\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64,
    "id": "XR-Y9RzuDNoC"
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_23Z_do-faF"
   },
   "source": [
    "## \ud83d\udd04  Loading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6V-f2NoDNoD"
   },
   "source": [
    "Let's first load the [`Helsinki-NLP/opus-mt-en-it` translation model](https://huggingface.co/Helsinki-NLP/opus-mt-en-it)  and its tokenizer.\n",
    "\n",
    "We use the pretrained version available on Hugging Face.\n",
    "\n",
    "The HuggingFace library provides the `AutoModel` class for the loading.\n",
    "> The AutoModel class is a convenient way to load an architecture without needing to know the exact model class name because there are many models available. It automatically selects the correct model class based on the configuration file. You only need to know the task and checkpoint you want to use.  *Source:* [[1]](#r1) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yqGEXRvDNoD"
   },
   "source": [
    "> \u26a0\ufe0f However, the `AutoModel` class doesn't include the specific heads for the tasks that `Helsinki-NLP/opus-mt-en-it` was trained on (translation) [[2]](#r2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blO-tUbdDNoD"
   },
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px;\">\n",
    "  \ud83d\udcd6 The <code>AutoModel</code> class is a flexible base class for loading pretrained models.\n",
    "  The model <code>Helsinki-NLP/opus-mt-en-it</code> is trained for <strong>sequence-to-sequence task</strong> (translation),\n",
    "  so using <code>AutoModel</code> would load only the core architecture,\n",
    "  <strong>without the generation head</strong>, and it wouldn't support <code>.generate()</code>.\n",
    "  Instead, you should use the appropriate <strong>task-specific auto class</strong>.<br><br>\n",
    "  \ud83d\udc49 For our case study, we will use the <code>AutoModelForSeq2SeqLM</code> class.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X77QKwkcDNoD"
   },
   "source": [
    "This class is part of a broader family of task-specific `AutoModel` classes:\n",
    "\n",
    "- `AutoModelForCausalLM` \u2013 for causal language modeling (e.g., GPT-2). They are decoder-only models.\n",
    "- `AutoModelForMaskedLM` \u2013 for masked language modeling (e.g., BERT). They are encoder-only models.\n",
    "- `AutoModelForTokenClassification` \u2013 for classification tasks (e.g., NER), where we classify each token in the input text.\n",
    "- `AutoModelForSeq2SeqLM` \u2013 for sequence-to-sequence tasks (e.g., Marian, BART, mBART)\n",
    "- `...`\n",
    "\n",
    "You can read the entire list in the [documentation](https://huggingface.co/docs/transformers/en/model_doc/auto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkoHLMG-DNoE",
    "outputId": "edc914fc-5458-47e1-8ce6-38cf14432d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.marian.modeling_marian.MarianMTModel'>\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# load the model\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-it'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# print the type of the model\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5sB7aS7DNoE"
   },
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px;\">\n",
    " \ud83d\udd0d By examining the model type, we can see that <code>'Helsinki-NLP/opus-mt-en-it'</code> is based on the <strong>MarianMTModel</strong> architecture. While <code>AutoModelForSeq2SeqLM</code> is a <b> generic class </b> that automatically selects the appropriate model architecture, we can use the more specific class <code>MarianMTModel</code>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NakhRF5tDNoE"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-it'\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sW5NfqoDNoE"
   },
   "source": [
    "## \ud83d\udc40 Inspect the Model's Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOhcVuzADNoE"
   },
   "source": [
    "Let\u2019s explore the model\u2019s **configuration** to better understand its structure and hyperparameters.  \n",
    "You can access it directly via the `config` attribute:\n",
    "```python\n",
    "model.config\n",
    "```\n",
    "If you prefer, you can also view the full configuration file on the official Hugging Face model page:\n",
    "https://huggingface.co/Helsinki-NLP/opus-mt-en-it/blob/main/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "en_SnFtcDNoE",
    "outputId": "2f64c74f-f4fe-4bc0-d921-22cdc4bfeba8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_num_labels\": 3,\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"swish\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"MarianMTModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      80034\n",
       "    ]\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_attention_heads\": 8,\n",
       "  \"decoder_ffn_dim\": 2048,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 80034,\n",
       "  \"decoder_vocab_size\": 80035,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 8,\n",
       "  \"encoder_ffn_dim\": 2048,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 0,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_length\": 512,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"marian\",\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": false,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 80034,\n",
       "  \"scale_embedding\": true,\n",
       "  \"share_encoder_decoder_embeddings\": true,\n",
       "  \"static_position_embeddings\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 80035\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It0Hh9HdDNoE"
   },
   "source": [
    "**General Structure**\n",
    "\n",
    "```python\n",
    "\"model_type\": \"marian\"\n",
    "\"architectures\": [\"MarianMTModel\"]\n",
    "\"is_encoder_decoder\": true\n",
    "```\n",
    "**Model Structure**\n",
    "\n",
    "```python\n",
    "\"encoder_layers\": 6,\n",
    "\"decoder_layers\": 6,\n",
    "\"encoder_attention_heads\": 8,\n",
    "\"decoder_attention_heads\": 8,\n",
    "\"d_model\": 512,\n",
    "\"encoder_ffn_dim\": 2048,\n",
    "\"decoder_ffn_dim\": 2048\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMav1h9zDNoF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## \ud83d\udd0d Exploring the Model's Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg5eNdsYDNoF"
   },
   "source": [
    "You can print the model to take a look at its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1718959898745,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "height": 30,
    "id": "eoFkdTd6_g5o",
    "outputId": "bdcfde9f-28b7-4f43-ec0c-32c16677a776",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(80035, 512, padding_idx=80034)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(80035, 512, padding_idx=80034)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(80035, 512, padding_idx=80034)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=80035, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeElOawfDNoF"
   },
   "source": [
    "From the architecture, you can see that the model has:\n",
    "- a `shared` Embedding layer;\n",
    "- an `encoder`;\n",
    "- a `decoder`;\n",
    "- a final `lm_head`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pq7e7e0JDNoF"
   },
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZnYpf7UDNoF"
   },
   "source": [
    "if you want to look at the embedding layer in the model, you can access it using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30,
    "id": "-SzW_173DNoF",
    "outputId": "c052b7cd-ca20-4c2d-e058-1ed47d99e28f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(80035, 512, padding_idx=80034)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wpu-tW7hDNoF"
   },
   "source": [
    "| Parameter           | Meaning                                                                                          |\n",
    "|---------------------|--------------------------------------------------------------------------------------------------|\n",
    "| `80035`             | The **vocabulary size**: The number of unique token IDs the model knows. It can embed 80,035 different tokens. |\n",
    "| `512`               | The **embedding dimension**: Each token is mapped to a vector of 512 floating-point values.      |\n",
    "| `padding_idx=80034` | The token ID used for **padding** (`<pad>`). Its embedding won't be updated during training and is treated specially. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw6xpFO1DNoF"
   },
   "source": [
    "The model uses a shared embedding layer for both encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qdq4TNHXDNoF"
   },
   "source": [
    "To check if the `shared` embedding layer is really used by both the encoder and the decoder,  \n",
    "\n",
    "we can compare the ID of `model.model.shared` with the IDs of the embedding layers used in the encoder (`model.model.encoder.embed_tokens`)  \n",
    "\n",
    "and the decoder (`model.model.decoder.embed_tokens`).  \n",
    "\n",
    "If the IDs are the same, it means they all point to the same embedding layer \u2014 confirming that it\u2019s shared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFZAh8_dDNoF",
    "outputId": "b97a150e-e9b4-4624-e18f-ab0e830b75f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(model.model.shared) == id(model.model.encoder.embed_tokens) and id(model.model.shared) == id(model.model.decoder.embed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygKzcV41DNoG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Encoder Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MT9RPPUZDNoG"
   },
   "source": [
    "The MarianEncoder includes six encoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30,
    "id": "G4u5RPzqDNoG",
    "outputId": "96ad4af2-d1fc-4041-f3e4-735aca2e39eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianEncoder(\n",
       "  (embed_tokens): Embedding(80035, 512, padding_idx=80034)\n",
       "  (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x MarianEncoderLayer(\n",
       "      (self_attn): MarianAttention(\n",
       "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (activation_fn): SiLU()\n",
       "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ2ZdTrjDNoG"
   },
   "source": [
    "The code on the left matches the diagram on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 2534,
     "status": "ok",
     "timestamp": 1751475238930,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "jE_GQDftDt_x",
    "outputId": "ac0f2628-1f72-4396-b4a1-9437ce15357c"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/marian-encoder.png\" alt=\"Marian Encoder\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOYC66WRDNoG"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG_Xc1ddDNoG"
   },
   "source": [
    "MarianDecoder includes six decoedr layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30,
    "id": "5mZNRf1fDNoG",
    "outputId": "3370f8a3-c177-450c-da68-1f4856595d48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianDecoder(\n",
       "  (embed_tokens): Embedding(80035, 512, padding_idx=80034)\n",
       "  (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x MarianDecoderLayer(\n",
       "      (self_attn): MarianAttention(\n",
       "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (activation_fn): SiLU()\n",
       "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): MarianAttention(\n",
       "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4rEnRfIDNoG"
   },
   "source": [
    "The code on the left matches the diagram on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1751475271695,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "dts_N9k2D1qZ",
    "outputId": "9e0ea542-78a6-4d62-a40f-0ba1e546e5d8"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/marian-decoder.png\" alt=\"Marian Decoder\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt18Lv1cDNoK"
   },
   "source": [
    "### Language Modeling Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us1Jwk56DNoK"
   },
   "source": [
    "The decoder\u2019s output vector is passed to this final  `lm_head`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30,
    "id": "FuFmHFupDNoK",
    "outputId": "cc26be8c-d7fe-4e7a-eba4-25a27c762a50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=80035, bias=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1751475405173,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "OKjPRePuD_WJ",
    "outputId": "6a3bcb7e-cf03-4140-978a-837a9f068be8"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/lm-head.png\" alt=\"LM Head\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbWunSAEDNoK"
   },
   "source": [
    "- The decoder\u2019s output vector is passed to this final lm_head.\n",
    "- Then a softmax function turns these scores into probabilities.\n",
    "- The model selects the most likely word (e.g., \u201cGatto\u201d) based on these probabilities.\n",
    "- It transforms the model\u2019s internal output (vector of size 512) into a vector of size 80,035.\n",
    "- That large number (80,035) is the vocabulary size \u2013 every possible word or subword the model can generate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS7FtU_KDNoL"
   },
   "source": [
    "## \ud83d\udd20  Machine Translation English-to-Italian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JtO_tAUDNoL"
   },
   "source": [
    "This section, shows how you can translate a sentence using the APIs provided by Hugging Face.\n",
    "\n",
    "If you want to translate an English sentence into Italian using the <code>Helsinki-NLP/opus-mt-en-it</code>  model, you can follow these six steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-IIZqzvDNoL",
    "outputId": "53d5bb1f-7c38-4bbd-a1d1-7e33e13ab746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduzione: Adoro l'elaborazione del linguaggio naturale.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "# 1. Specify the name of the pre-trained English-to-Italian translation model\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-it'\n",
    "\n",
    "# 2. Load the tokenizer and model associated with the specified translation model\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 3. Define the input sentence(s) to be translated\n",
    "src_text = [\"I love natural language processing.\"]\n",
    "\n",
    "# 4. Tokenize the input text and returns it as input tensors for the model\n",
    "input_ids = tokenizer(src_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 5. Generate the translated output tokens using the model\n",
    "#translated = model.generate(**encoded)\n",
    "# it gives us tokens (numbers), not words yet\n",
    "translated = model.generate(input_ids=input_ids.input_ids, attention_mask=input_ids.attention_mask)\n",
    "\n",
    "\n",
    "# 6. Decode the generated tokens into human-readable text, skipping any special tokens\n",
    "translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "print(\"Traduzione:\", translated_text[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd3D1FwKDNoL"
   },
   "source": [
    "\ud83d\udc49 This code showed how a sentence can be translated using the API provided by Hugging Face.\n",
    "\n",
    "Now, let\u2019s take a closer look at how tokens are generated by the  `generate() ` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4-HPcwmDNoL"
   },
   "source": [
    "## \ud83e\udde9 Deep dive into Token Generation `model.generate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blNaX_N9DNoL"
   },
   "source": [
    "In this section, we simulate how the models generates one token at a time through the `generate()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeFzlK7XDNoL"
   },
   "source": [
    "The input for the encoder is the tokenized input text.\n",
    "\n",
    "The decoder input will be the sequence generated up to that point.\n",
    "\n",
    "For the first iteration, there is nothing already generated, so a special token must be specified to indicate the beginning of the sequence (BOS). For `Helsinki-NLP/opus-mt-en-it`, the token will be `<pad_token>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A_2Zy3-DNoL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "### --- We define the encoder input --- ###\n",
    "\n",
    "# Define the English sentence we want to translate.\n",
    "input_sentence = \"I love natural language processing.\"\n",
    "\n",
    "# We use the tokenizer to convert the sentence into tokens (numbers).\n",
    "# 'return_tensors=\"pt\"' means we want the output as PyTorch tensors.\n",
    "tokens = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "### --- We manually create the first decoder input. --- ###\n",
    "\n",
    "# It starts with the padding token as a placeholder (in real generation, this might be a start token).\n",
    "# The model will use this to begin generating the first word.\n",
    "decoder_input_ids = torch.tensor([[ tokenizer.pad_token_id ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkN1tFaVDNoL",
    "outputId": "b47ba503-e670-4bb0-bc0d-27aa9033064d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder's input\n",
      "tensor([[  22,  722, 1552, 2413, 3795,    2,    0]])\n",
      "\n",
      "Decoder's input\n",
      "tensor([[80034]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder's input\")\n",
    "print(tokens[\"input_ids\"])\n",
    "\n",
    "print()\n",
    "print(\"Decoder's input\")\n",
    "print(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGX5K6O0DNoL"
   },
   "source": [
    "To generate the first output token (after `<pad_token>`), we call the `forward()` method with the input text and the decoder's input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnfwi91uDNoL"
   },
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px;\">\n",
    "  <strong>\ud83d\udcd6</strong> The <code>forward</code> method is the core function that takes the input sentence and decoder input, runs them through the model, and returns the raw predictions for the next token.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8G4996mDNoL",
    "outputId": "27eacc2d-3ee6-4e8a-c503-b143c568251e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(\n",
    "    input_ids=tokens[\"input_ids\"],\n",
    "    attention_mask=tokens[\"attention_mask\"],\n",
    "    decoder_input_ids=decoder_input_ids\n",
    ")\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXtH8KBCDNoL"
   },
   "source": [
    "The `forward` method returns a dictionary that contains:\n",
    "\n",
    "- `logits`: These are raw (non-normalized) score generated by the model before applying any activation function. The logits are the output of the linear layer (`lm_head`) that maps the decoder's output to the vocabulary size. The logits are used to compute the probabilities of each token.\n",
    "\n",
    "- `past_key_values`: This helps the model remember what it has already generated. It's used to make the next steps faster during text generation. Since the decoder is autoregressive\u2014it generates one token at a time and only looks at previous tokens\u2014the predictions for earlier tokens stay the same. That means we don\u2019t need to recalculate them. Instead, we can save (`cache`) those values as past_key_values and reuse them the next time we call the model. This makes generation more efficient.\n",
    "\n",
    "- `encoder_last_hidden_state`: The hidden states of the last layer of the encoder. This is not used for generating the next token, but it can be useful for other tasks (e.g., summarization) or for debbuging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETKF2wTPDNoM",
    "outputId": "2205d158-b475-4d3e-c57c-6106473b89e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 80035])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTTRTjpcDNoM"
   },
   "source": [
    "Shape: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "- `batch_size` = 1 (one sentence),\n",
    "\n",
    "- `sequence_length` = 1 (we're predicting the first token only),\n",
    "\n",
    "- `vocab_size` = number of possible tokens the model knows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMEfgLXzDNoM"
   },
   "source": [
    "Now that we have the logits, how do we find the most likely next word?\n",
    "To do this, we need to turn the logits into probabilities for each token. One way to select the next word is by using the **greedy decoding technique**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0U_Y5OUDNoM"
   },
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px;\">\n",
    "  \ud83d\udcd6 The <b>Greedy decoding </b> is a simple text generation method where, at each step, the model chooses the token with the\n",
    "  highest probability \u2014 the one it thinks is most likely \u2014 without considering any other possibilities.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwmeWNBgDNoM",
    "outputId": "1902d1c0-a2ad-4750-a92b-b9835966d7b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2617, -4.9812, -0.3660,  ..., -4.9893, -5.0309,  0.0000],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first 0 refers to the first sentence in the batch.\n",
    "# The second 0 refers to the first token in the output sequence.\n",
    "# The output returns a vector of size vocab_size: one score for every possible wrod in the vocabulary\n",
    "output.logits[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iAK2XRdDNoM",
    "outputId": "e28d6c85-c94a-4395-8a99-fad6bdd35cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max probability token ID: 1235 | Logit value: 8.184107780456543\n",
      "Corresponding token (Word): Ad\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the index of the highest logit value: takes the highest scoring token for the first sentence (greedy decoding at step 1)\n",
    "max_proba_token = output.logits[0,0].argmax()\n",
    "# Extract the logit value corresponding to the selected token\n",
    "\n",
    "logit_value = output.logits[0, 0, max_proba_token]\n",
    "print(f\"Max probability token ID: {max_proba_token.item()} | Logit value: {logit_value.item()}\")\n",
    "print(\"Corresponding token (Word):\", tokenizer.decode(max_proba_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtLicOSTDNoM"
   },
   "source": [
    "We add the predicted token to our `decoder_input_ids` so that we can generate the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dyo6wU3dDNoM",
    "outputId": "a2bdd7bc-3b21-42a6-bff7-d5dfa99f4e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[80034,  1235]])\n"
     ]
    }
   ],
   "source": [
    "# .view() acts like .reshape()\n",
    "decoder_input_ids = torch.hstack([decoder_input_ids, max_proba_token.view(1, 1)])\n",
    "print(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5IhMaigDNoM"
   },
   "source": [
    "We call the `forward` method to predict the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yg4Chi1DNoM",
    "outputId": "f4d83625-16b7-4440-ef0c-cd2da9c9a3cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (batch_size, sequence_length, vocab_size)\n",
      "torch.Size([1, 2, 80035])\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(**tokens, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "print(\"Shape: (batch_size, sequence_length, vocab_size)\")\n",
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ty1_P9vDNoM"
   },
   "source": [
    "Now, the `sequence_length` is 2,  because we\u2019ve generated two tokens.\n",
    "\n",
    "We can confirm this by decoding both token predictions and seeing the output. Let\u2019s extract the most probable token at each position, map the token IDs back to their string representations, and decode the full sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-di5mmNzDNoM",
    "outputId": "02bf8eec-e31d-47c0-e446-008a3cfc29f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids:  tensor([1235, 3351])\n",
      "Mapped tokens:  ['\u2581Ad', 'oro']\n",
      "Decoded string (word):  Adoro\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the full vocabulary from the tokenizer: a dictionary mapping token strings to their IDs\n",
    "vocabulary = tokenizer.get_vocab()\n",
    "# Create a reverse mapping from token IDs to token strings (for easier lookup and display)\n",
    "reverse_vocab = { v: k for k, v in vocabulary.items() }\n",
    "\n",
    "# 0:  first batch\n",
    "max_proba_tokens = output.logits[0].argmax(axis=1)\n",
    "print(\"Token ids: \", max_proba_tokens)\n",
    "print(\"Mapped tokens: \", list(map(reverse_vocab.get, max_proba_tokens.tolist())))\n",
    "print(\"Decoded string (word): \", tokenizer.decode(max_proba_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJMW-VqJDNoM"
   },
   "source": [
    "Instead of manually generating one token at a time, we let the model generate the whole output in one go.\n",
    "This uses greedy decoding with randomness (do_sample=True), which picks the most likely token at each step, but adds some variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H35YzvbFDNoN",
    "outputId": "e66f29bb-2a73-4d07-85d6-51e384e46d44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<pad> Adoro l'elaborazione del linguaggio naturale.</s>\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greedy deoding\n",
    "tokenizer.batch_decode(model.generate(**tokens, do_sample=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtmBXJHtDNoO"
   },
   "source": [
    "Finally, let's manually simulate what `generate()` does behind the scenes using greedy decoding. Using greedy decoding, we\u2019ll generate a sequence one token at a time by repeatedly feeding the model\u2019s output back into itself. At each step, we\u2019ll pick the most likely next token and continue building the sequence until we reach the end-of-sequence token or a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPqRtv8tDNoO",
    "outputId": "e54e9d6d-e976-4529-f596-0632411c0c77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Ad\n",
      "Step 2: Adoro\n",
      "Step 3: Adoro l\n",
      "Step 4: Adoro l'\n",
      "Step 5: Adoro l'elaborazione\n",
      "Step 6: Adoro l'elaborazione del\n",
      "Step 7: Adoro l'elaborazione del linguaggio\n",
      "Step 8: Adoro l'elaborazione del linguaggio naturale\n",
      "Step 9: Adoro l'elaborazione del linguaggio naturale.\n",
      "Step 10: Adoro l'elaborazione del linguaggio naturale.</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoder_input_ids = torch.tensor([[ tokenizer.pad_token_id ]])\n",
    "\n",
    "max_length = 15\n",
    "i = 0\n",
    "\n",
    "while i < max_length and decoder_input_ids[0,-1] != tokenizer.eos_token_id:\n",
    "    output = model(**tokens, decoder_input_ids=decoder_input_ids)\n",
    "    max_proba_tokens = output.logits[0].argmax(axis=1)\n",
    "    print(f\"Step {i+1}: {tokenizer.decode(max_proba_tokens)}\")\n",
    "    decoder_input_ids = torch.hstack([decoder_input_ids, max_proba_tokens[-1].view(1, 1)])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfpkYf9KDNoO",
    "outputId": "c7fce2d7-353d-4d55-b6f5-9c3a56d439a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<pad> Adoro l'elaborazione del linguaggio naturale.</s>\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tokens = model.generate(**tokens, max_length=max_length)\n",
    "tokenizer.batch_decode(out_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzI3Fj8rDNoO",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## \ud83d\udee0\ufe0f Customizing Model Architecture with Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZpXKslsDNoO"
   },
   "source": [
    "Hugging Face Transformers library provides powerful ways to customize model behavior through their configuration classes.\n",
    "\n",
    "You can modify the model's configuration class to change how a model is built.  \n",
    "The configuration defines the architecture of the model, including attributes like the number of hidden layers, attention heads, hidden size, dropout probabilities, and more.\n",
    "\n",
    "There are two main ways to use configurations:\n",
    "\n",
    "1. **Start from scratch** by creating a custom configuration.  \n",
    "   In this case, the model is initialized with random weights and must be trained before use it.\n",
    "\n",
    "2. **Modify a pre-trained model's configuration**, which allows you to tweak how the model behaves (e.g., enable attention outputs or change generation settings) without losing the pretrained weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lptSkP92DNoO"
   },
   "source": [
    "#### 1. Starting From Scratch with a Custom Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js13P-F1DNoO"
   },
   "source": [
    "If you want full control over the model's architecture \u2014 for example, to experiment with different layer sizes or number of heads\n",
    "\u2014 you can create a configuration from scratch.\n",
    "\n",
    "This approach initializes the model with **random weights**, meaning you must **train it from scratch** before it's usable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwYVaYmpDNoO",
    "outputId": "ebd2d788-fa89-418c-ce26-631446a0e3a8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized from scratch with custom config:\n",
      "MarianConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 58100,\n",
      "  \"decoder_vocab_size\": 59590,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 58100,\n",
      "  \"scale_embedding\": false,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59590\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianConfig, MarianMTModel\n",
    "\n",
    "# Define a new configuration (same structure as MarianMT)\n",
    "custom_config = MarianConfig(\n",
    "    vocab_size=59590,\n",
    "    d_model=512,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=6,\n",
    "    encoder_attention_heads=8,\n",
    "    decoder_attention_heads=8\n",
    ")\n",
    "\n",
    "# Create the model with random weights\n",
    "# He does not have pre-trained weights, so he cannot translate yet!\n",
    "model_from_scratch = MarianMTModel(config=custom_config)\n",
    "\n",
    "print(\"Model initialized from scratch with custom config:\")\n",
    "print(model_from_scratch.config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxACuqaIDNoO"
   },
   "source": [
    "#### 2. Modifying a pre-trained model's configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJoopwrJDNoO"
   },
   "source": [
    "In this example, we modify the model's configuration to:\n",
    "\n",
    "- Get the **attention weights**, which show what parts of the input the model focuses on at each step.\n",
    "- Get the **hidden states**, which are the internal representations of each token at every layer of the model.\n",
    "\n",
    "This is useful when you want to **analyze or better understand** how the model works internally.\n",
    "\n",
    "We use the pretrained MarianMT model (`Helsinki-NLP/opus-mt-en-it`) and translate a simple sentence from English to Italian.  \n",
    "The model weights stay the same \u2014 we\u2019re only changing the **output behavior**, not the training.\n",
    "\n",
    "Then, we use:\n",
    "- `model.generate()` to get the translated sentence.\n",
    "- `model()` (the forward pass) to explore the attention maps and hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWu0qJH2DNoO",
    "outputId": "1453164c-c367-4c7e-a60d-99fa656edaec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: The cat eats a mouse\n",
      "Translated sentence: Il gatto mangia un topo\n",
      "Keys in output: odict_keys(['logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'])\n",
      "Number of encoder hidden states: 7\n",
      "Shape of attention from encoder layer 0: torch.Size([1, 8, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# 1. Define the name of the pretrained model\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-it\"\n",
    "\n",
    "# 2. Load the tokenizer and the pretrained model\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 3. Modify the configuration to enable extra outputs\n",
    "# These changes do not affect the pretrained weights\n",
    "model.config.output_attentions = True\n",
    "model.config.output_hidden_states = True\n",
    "model.config.return_dict = True  # Needed to access outputs as a dictionary\n",
    "\n",
    "# 4. Define the input sentence to translate\n",
    "sentence = \"The cat eats a mouse\"\n",
    "print(\"Original sentence:\", sentence)\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")  # Convert text to tensors\n",
    "\n",
    "# 5. Use the forward() method to manually run the model and get full outputs\n",
    "output = model(**inputs, decoder_input_ids=inputs[\"input_ids\"])\n",
    "\n",
    "# 6. Use generate() to get the translated sentence\n",
    "translated_ids = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 7. Print the final translated sentence\n",
    "print(\"Translated sentence:\", translated_text)\n",
    "\n",
    "# 8. Optional: print extra info from the model outputs\n",
    "print(\"Keys in output:\", output.keys())  # Show what\u2019s inside the output\n",
    "print(\"Number of encoder hidden states:\", len(output.encoder_hidden_states))  # How many layers\n",
    "print(\"Shape of attention from encoder layer 0:\", output.encoder_attentions[0].shape)  # Attention matrix size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7NQ4yQNDNoP"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtoFLdo3DNoP"
   },
   "source": [
    "<a name=\"r1\">[1]</a> https://huggingface.co/docs/transformers/en/models\n",
    "\n",
    "<a name=\"r2\">[2]</a> https://huggingface.co/docs/transformers/en/model_doc/marian\n",
    "\n",
    "<a name=\"r2\">[3]</a> https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb#scrollTo=4D89wY_Z8Cg7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Uw_WJfHDNoP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}