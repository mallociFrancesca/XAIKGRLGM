{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5061d623",
   "metadata": {
    "id": "5061d623",
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "# Introduction to PEARLM: Code Explanation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142e24a",
   "metadata": {
    "id": "0142e24a"
   },
   "source": [
    "This notebook is designed as an introductory guide to help students understand the basic concepts needed to implement a Path Language Model using the Hopwise library. Through the example of the PEARLM (Path-Enhanced Autoregressive Language Model) [[1]](#r1) model, it provides a practical explanation of how to build an explainable recommendation system based on knowledge graphs.\n",
    "\n",
    "\n",
    "\n",
    "The notebook is organized into two main sections:\n",
    "- **1\ufe0f\u20e3 Understanding the PEARLM Class: Code Workflow**: This section explains the implementation of the PEARLM class, including its core functions and architecture.\n",
    "- **2\ufe0f\u20e3 Decoding Stage: Knowledge Graph-Constrained Decoding (KGCD)**: This section describes the decoding process constrained by the knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PVfmmVLaE3bv",
   "metadata": {
    "id": "PVfmmVLaE3bv",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### \u2699\ufe0f Setup Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CQP1ec1EE1Gn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQP1ec1EE1Gn",
    "outputId": "005d5546-528a-43b9-ae46-fae18498edd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9db0f",
   "metadata": {
    "id": "cff9db0f"
   },
   "source": [
    "# 1\ufe0f\u20e3 Understanding the PEARLM Class: Code Workflow\n",
    "\n",
    "This section outlines the core functions in the PEARLM model class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ab7d3",
   "metadata": {
    "id": "091ab7d3"
   },
   "source": [
    "<div style=\"background-color:#f0f4f8; border-left: 5px solid #4a90e2; padding:15px; margin:10px 0; border-radius:8px;\">\n",
    "<strong> PEARLM </strong> is a path-language-modeling recommender. It learns the sequence of entity-relation triplets from a knowledge graph as a next-token prediction task.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_wP1B1o6dNbU",
   "metadata": {
    "id": "_wP1B1o6dNbU"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/pearlm-arch.png\" alt=\"PEARLM Architecture\" width=\"900\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48626843",
   "metadata": {
    "id": "48626843"
   },
   "source": [
    "**Table of Contents:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a6bcf",
   "metadata": {
    "id": "226a6bcf"
   },
   "source": [
    "**\ud83d\udce6 [0. Packages](#S0)**\n",
    "- Import necessary libraries and modules\n",
    "\n",
    "**\ud83e\uddf1 [1. Model Initialization (`__init__`)](#S1)**\n",
    "- Initializes the PEARLM model using a transformer (pretrained GPT-2 model) with custom settings.\n",
    "- Defines a fixed sequence template of token types (e.g., `[<SPECIAL>, <ENTITY>, <RELATION>, <ENTITY>, ..., <SPECIAL>]`).\n",
    "- Defines the loss function for the next-prediction token.\n",
    "\n",
    "**\ud83d\udd01 [2. Forward Pass (`forward`)](#S2)**\n",
    "- Defines how the model processes an input during:\n",
    "    - `training` (when `labels` are given): it also calculates the cross-entropy loss.\n",
    "    - `inference` (when generating predictions): it directly returns the predicted tokens.\n",
    "\n",
    "**\ud83d\udd2e [3. Prediction Interface (`predict`)](#S3)**\n",
    "- A convenience method for inference that internally calls `forward()`.\n",
    "\n",
    "\n",
    "**\u2728 [4. Generate Interface (`generate`)](#S4)**\n",
    "- Responsible for generating sequences of tokens based on the input provided, incorporating custom constraints like Knowledge Graph-Constrained Decoding (KGCD).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8868aa5",
   "metadata": {
    "id": "e8868aa5",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"S0\"></a>\n",
    "## \ud83d\udce6 0. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d53ca",
   "metadata": {
    "id": "141d53ca"
   },
   "outputs": [],
   "source": [
    "# math\n",
    "from enum import Enum\n",
    "\n",
    "# typing\n",
    "from typing import Optional, Union\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# HuggingFace library\n",
    "from transformers import AutoConfig, GPT2LMHeadModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "# Hopwise library\n",
    "from hopwise.data import Interaction\n",
    "from hopwise.model.abstract_recommender import ExplainablePathLanguageModelingRecommender\n",
    "from hopwise.utils import PathLanguageModelingTokenType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fb63d",
   "metadata": {
    "id": "c87fb63d"
   },
   "source": [
    "<a id=\"S1\"></a>\n",
    "## \ud83e\uddf1 1. Model Initialization (`__init__`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5zzQahvFFzXq",
   "metadata": {
    "id": "5zzQahvFFzXq"
   },
   "source": [
    "Internally, hopwise manages the token types used by language models through an enumeration defined as follows, which PEARLM uses to assign a unique ID to each token type:\n",
    "- S => SPECIAL\n",
    "- E => ENTITY (any KG node is labelled with this token type, including, users, and items)\n",
    "- R => RELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272ee71",
   "metadata": {
    "id": "1272ee71"
   },
   "outputs": [],
   "source": [
    "class PathLanguageModelingTokenType(Enum):\n",
    "    \"\"\"Type of tokens in paths for Path Language Modeling.\n",
    "\n",
    "    - ``SPECIAL``: Special tokens, like start and end of a path.\n",
    "    - ``ENTITY``: Entity tokens.\n",
    "    - ``RELATION``: Relation tokens.\n",
    "    - ``USER``: User tokens.\n",
    "    - ``ITEM``: Item tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    SPECIAL = (\"S\", 0)\n",
    "    ENTITY = (\"E\", 1)\n",
    "    RELATION = (\"R\", 2)\n",
    "    USER = (\"U\", 3)\n",
    "    ITEM = (\"I\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fab6be",
   "metadata": {
    "id": "69fab6be"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PEARLM(ExplainablePathLanguageModelingRecommender, GPT2LMHeadModel):\n",
    "    def __init__(self, config, dataset):\n",
    "\n",
    "        #  Initialize the constructor of the parent class\n",
    "        ExplainablePathLanguageModelingRecommender.__init__(self, config, dataset)\n",
    "\n",
    "        # Load settings from the configuration parameters\n",
    "        self.use_kg_token_types = config[\"use_kg_token_types\"]\n",
    "\n",
    "         # Load the pre-initialized tokenizer from the dataset\n",
    "        tokenizer = dataset.tokenizer\n",
    "\n",
    "         # Configure the causal language model (based on distilGPT2) with custom settings\n",
    "        transformers_config = AutoConfig.from_pretrained(\n",
    "            \"distilgpt2\",\n",
    "            **{\n",
    "                \"vocab_size\": self.n_tokens,  # Defines the different tokens that can be represented by the inputs_ids passed to the forward method. Shortcut for len(tokenizer)\n",
    "                \"n_ctx\": config[\"context_length\"], # Maximum number of tokens that the model can account for when processing a response\n",
    "                \"n_positions\": config[\"context_length\"], # Maximum number of positions (same as context length)\n",
    "                \"pad_token_id\": tokenizer.pad_token_id, # ID of the special tokens used by the tokenizer\n",
    "                \"bos_token_id\": tokenizer.bos_token_id, # ID of the special tokens used by the tokenizer\n",
    "                \"eos_token_id\": tokenizer.eos_token_id, # ID of the special tokens used by the tokenizer\n",
    "                \"n_embd\": config[\"embedding_size\"],  # Embedding dimension\n",
    "                \"n_head\": config[\"num_heads\"], # Number of attention heads\n",
    "                \"n_layer\": config[\"num_layers\"], # Number of transformer layers\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Initialize the model with the architecture defined in \"transformers_config\"\n",
    "        GPT2LMHeadModel.__init__(self, transformers_config)\n",
    "\n",
    "        # Move the model on CPU or GPU\n",
    "        self.to(config[\"device\"])\n",
    "\n",
    "        # Path Language Modeling Token Type Template Definition\n",
    "        # This block sets up a template to differentiate between SPECIAL, ENTITY, and RELATION tokens in the KG path.\n",
    "        # It augments the tokenizer with three special tokens: <SPECIAL>, <ENTITY>, and <RELATION>,\n",
    "        # which will later be used to assign semantic roles to each position in the input path sequence.\n",
    "\n",
    "        # If you require the model to use token types\n",
    "        if self.use_kg_token_types:\n",
    "\n",
    "            # Save current vocabulary size before adding the new token types\n",
    "            prev_vocab_size = len(dataset.tokenizer)\n",
    "\n",
    "            # Unpack each enumeration value in type (str) and type_id (int) for the token type definition\n",
    "            spec_type, spec_type_id = PathLanguageModelingTokenType.SPECIAL.value\n",
    "            ent_type, ent_type_id = PathLanguageModelingTokenType.ENTITY.value\n",
    "            rel_type, rel_type_id = PathLanguageModelingTokenType.RELATION.value\n",
    "\n",
    "            # Define the tokens types to be added to the tokenizer: SPECIAL, ENTITY, RELATION\n",
    "            # These are used to identify the semantic role of each token in the path (e.g., user, movie, acted_in)\n",
    "            # Example: \"<ENTITY>\" might correspond to \"Tom Hanks\", \"<RELATION>\" to \"acted_in\"\n",
    "            token_types = [f\"<{token_type.name}>\" for token_type in [spec_type, ent_type, rel_type]] # <SPECIAL>, <ENTITY>, <RELATION>\n",
    "\n",
    "\n",
    "            # Add the type tokens to the tokenizer so they can be referenced like normal tokens\n",
    "            for token_type in token_types:\n",
    "                dataset.tokenizer.add_tokens(token_type)\n",
    "\n",
    "            # Update the vocabulary size registered by the attribute `n_tokens` after adding new tokens\n",
    "            self.n_tokens = len(dataset.tokenizer)\n",
    "\n",
    "\n",
    "            # Retrieve the corresponding integer token IDs of the added token types\n",
    "            # Since we added them after the original vocabulary, we shift their values accordingly\n",
    "            spec_type, ent_type, rel_type = (\n",
    "                spec_type + prev_vocab_size,\n",
    "                ent_type + prev_vocab_size,\n",
    "                rel_type + prev_vocab_size,\n",
    "            )\n",
    "\n",
    "            # Build a fixed type pattern (template) for a KG path:\n",
    "            # e.g., With path_hop_length = 2, the structure would be:\n",
    "            # [<SPECIAL>, <ENTITY>, <RELATION>, <ENTITY>, <RELATION>, <ENTITY>, <SPECIAL>]\n",
    "            # This will be used to inform the model which kind of token is expected at each position\n",
    "            self.token_type_ids = torch.LongTensor(\n",
    "                [spec_type, ent_type] + [rel_type, ent_type] * dataset.path_hop_length + [spec_type]\n",
    "            ).to(config[\"device\"])\n",
    "\n",
    "            # Resize the transformer's token embedding matrix to include the new tokens type we just defined\n",
    "            self.transformer.resize_token_embeddings(len(dataset.tokenizer))\n",
    "\n",
    "        # Define the loss function used during training: Cross Entropy for next-token prediction\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Final setup step required by HuggingFace models to complete module initialization\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, input_ids, **kwargs):\n",
    "        return self.forward(input_ids, **kwargs)\n",
    "\n",
    "\n",
    "    def generate(self, inputs, **kwargs):\n",
    "        kwargs[\"logits_processor\"] = self.logits_processor_list\n",
    "        kwargs[\"num_return_sequences\"] = kwargs.pop(\"paths_per_user\")\n",
    "        return super(GPT2LMHeadModel, self).generate(**inputs, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ffbcc",
   "metadata": {
    "id": "f79ffbcc"
   },
   "source": [
    "#### Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310de2c8",
   "metadata": {
    "id": "310de2c8"
   },
   "outputs": [],
   "source": [
    "class PEARLM(ExplainablePathLanguageModelingRecommender, GPT2LMHeadModel):\n",
    "    def __init__(self, config, dataset):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e039c",
   "metadata": {
    "id": "3d0e039c"
   },
   "source": [
    "This class inherits from:\n",
    "\n",
    "- **`ExplainablePathLanguageModelingRecommender`**: It is an abstract class from hopwise library that unifies PathLanguageModelingRecommender and ExplainableRecommender interfaces.\n",
    "  All the path-language-modeling models that generate explanations should implement this class.\n",
    "  It provides methods and logic for:\n",
    "  - modeling user-item interactions as paths in the graph\n",
    "  - defining custom logit processors during inference\n",
    "  - postprocessing path sequences to be fed into the evaluation pipeline\n",
    "  - decoding token sequences into the explanation standard format\n",
    "  - exposing the explainability interface\n",
    "\n",
    "- **`GPT2LMHeadModel`**: the HuggingFace implementation of the GPT-2 language model, with a linear head on top for causal language modeling.  \n",
    "  This enables the model to generate sequences token-by-token in an autoregressive manner ([Documentation](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel)).\n",
    "\n",
    "The `__init__` method of the `PEARLM` class is responsible for configuring and initializing the model.\n",
    "It takes as args:\n",
    "- **`config`**: A configuration dictionary containing all necessary hyperparameters and settings for the model.\n",
    "- **`dataset`**: A structured dataset object that includes the tokenizer, knowledge graph paths, and other relevant data required for training and inference.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14826c38",
   "metadata": {
    "id": "14826c38"
   },
   "source": [
    "#### Class Inizialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db4651-863d-4bea-9fb4-fa78f1dc6211",
   "metadata": {
    "id": "a8db4651-863d-4bea-9fb4-fa78f1dc6211"
   },
   "outputs": [],
   "source": [
    "class PEARLM(ExplainablePathLanguageModelingRecommender, GPT2LMHeadModel):\n",
    "    def __init__(self, config, dataset):\n",
    "\n",
    "         # Load the pre-initialized tokenizer from the dataset\n",
    "        tokenizer = dataset.tokenizer\n",
    "\n",
    "         # Configure the causal language model (based on distilGPT2) with custom settings\n",
    "        transformers_config = AutoConfig.from_pretrained(\"distilgpt2\", {...})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80895fc1",
   "metadata": {
    "id": "80895fc1"
   },
   "source": [
    "This lines  `transformers_config = AutoConfig.from_pretrained(\"distilgpt2\", {...})`  creates a new model configuration starting from the pre-trained GPT-2 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e43b97",
   "metadata": {
    "id": "46e43b97"
   },
   "source": [
    "Args:\n",
    "\n",
    "| Parameter              | Description |\n",
    "|------------------------|-------------|\n",
    "| `vocab_size`           | Number of unique tokens, including all KG entities, relations, and special tokens (like `[BOS]`, `[EOS]`, `[PAD]`). |\n",
    "| `n_ctx`, `n_positions` | Maximum number of tokens in the input sequence (i.e., the maximum path length supported by the model). |\n",
    "| `pad_token_id`         | ID of the padding token used to align sequences to the same length. |\n",
    "| `bos_token_id`         | ID for the beginning-of-sequence token. It signals the start of a generated path. |\n",
    "| `eos_token_id`         | ID for the end-of-sequence token. It marks the end of a reasoning path. |\n",
    "| `n_embd`               | Dimensionality of the embeddings used for each token (i.e., the size of the hidden representation). |\n",
    "| `n_head`               | Number of attention heads in each Transformer block. Determines how many attention subspaces are used. |\n",
    "| `n_layer`              | Total number of Transformer layers (blocks) in the model. More layers = deeper model. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac775a",
   "metadata": {
    "id": "58ac775a"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2364c6",
   "metadata": {
    "id": "5e2364c6"
   },
   "source": [
    "#### Input Preparation: Token type sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab1a38",
   "metadata": {
    "id": "48ab1a38"
   },
   "source": [
    "This block sets up a **template** to differentiate between `SPECIAL`, `ENTITY`, and `RELATION` tokens in the KG path.\n",
    "\n",
    "It augments the tokenizer with three special tokens: `<SPECIAL>`, `<ENTITY>`, and `<RELATION>`, which will later be used to assign semantic roles to each position in the input path sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac22ff",
   "metadata": {
    "id": "c0ac22ff"
   },
   "outputs": [],
   "source": [
    "        # Path Language Modeling Token Type Template Definition\n",
    "        # This block sets up a template to differentiate between SPECIAL, ENTITY, and RELATION tokens in the KG path.\n",
    "        # It augments the tokenizer with three special tokens: <SPECIAL>, <ENTITY>, and <RELATION>,\n",
    "        # which will later be used to assign semantic roles to each position in the input path sequence.\n",
    "\n",
    "        # If you require the model to use token types\n",
    "        if self.use_kg_token_types:\n",
    "\n",
    "            # Save current vocabulary size before adding the new token types\n",
    "            prev_vocab_size = len(dataset.tokenizer)\n",
    "\n",
    "            # Unpack each enumeration value in type (str) and type_id (int) for the token type definition\n",
    "            spec_type, spec_type_id = PathLanguageModelingTokenType.SPECIAL.value\n",
    "            ent_type, ent_type_id = PathLanguageModelingTokenType.ENTITY.value\n",
    "            rel_type, rel_type_id = PathLanguageModelingTokenType.RELATION.value\n",
    "\n",
    "            # Define the tokens types to be added to the tokenizer: SPECIAL, ENTITY, RELATION\n",
    "            # These are used to identify the semantic role of each token in the path (e.g., user, movie, acted_in)\n",
    "            # Example: \"<ENTITY>\" might correspond to \"Tom Hanks\", \"<RELATION>\" to \"acted_in\"\n",
    "            token_types = [f\"<{token_type.name}>\" for token_type in [spec_type, ent_type, rel_type]] # <SPECIAL>, <ENTITY>, <RELATION>\n",
    "\n",
    "\n",
    "            # Add the type tokens to the tokenizer so they can be referenced like normal tokens\n",
    "            for token_type in token_types:\n",
    "                dataset.tokenizer.add_tokens(token_type)\n",
    "\n",
    "            # Update the vocabulary size registered by the attribute `n_tokens` after adding new tokens\n",
    "            self.n_tokens = len(dataset.tokenizer)\n",
    "\n",
    "\n",
    "            # Retrieve the corresponding integer token IDs of the added token types\n",
    "            # Since we added them after the original vocabulary, we shift their values accordingly\n",
    "            spec_type, ent_type, rel_type = (\n",
    "                spec_type + prev_vocab_size,\n",
    "                ent_type + prev_vocab_size,\n",
    "                rel_type + prev_vocab_size,\n",
    "            )\n",
    "\n",
    "            # Build a fixed type pattern (template) for a KG path:\n",
    "            # e.g., With path_hop_length = 2, the structure would be:\n",
    "            # [<SPECIAL>, <ENTITY>, <RELATION>, <ENTITY>, <RELATION>, <ENTITY>, <SPECIAL>]\n",
    "            # This will be used to inform the model which kind of token is expected at each position\n",
    "            self.token_type_ids = torch.LongTensor(\n",
    "                [spec_type, ent_type] + [rel_type, ent_type] * dataset.path_hop_length + [spec_type]\n",
    "            ).to(config[\"device\"])\n",
    "\n",
    "            # Resize the transformer's token embedding matrix to include the new tokens type we just defined\n",
    "            self.transformer.resize_token_embeddings(len(dataset.tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f814f32",
   "metadata": {
    "id": "7f814f32"
   },
   "source": [
    "----\n",
    "<a name=\"#S2\"></a>\n",
    "\n",
    "## \ud83d\udd01 2. Forward Pass (`forward`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2446c28",
   "metadata": {
    "id": "a2446c28"
   },
   "source": [
    "This function defines how the model processes an input during:\n",
    "\n",
    "- `training` (when `labels` are given): it also calculates the cross-entropy loss.\n",
    "- `inference` (when generating predictions): it directly returns the predicted tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b4dd9a",
   "metadata": {
    "id": "89b4dd9a"
   },
   "outputs": [],
   "source": [
    "# PEARLM extends the GPT2LMHeadModel class.\n",
    "# The \"forward\" method is overrided to include custom logic for handling type embeddings (ENTITY, RELATION, SPECIAL).\n",
    "def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None, # Indices of input sequence tokens in the vocabulary.\n",
    "        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None, # Cached (key,value) pairs to speed up autoregressive decoding\n",
    "        attention_mask: Optional[torch.FloatTensor] = None, # Prevents the model from giving weight to padding tokens (PAD)\n",
    "        token_type_ids: Optional[torch.LongTensor] = None, # Segment token indices to indicate first and second portions of the inputs.\n",
    "        position_ids: Optional[torch.LongTensor] = None,  # Positional indices for each token\n",
    "        head_mask: Optional[torch.FloatTensor] = None, # Disable specific attention heads in the model\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None, # Embedded representation of the input_ids\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None, # Used in encoder-decoder settings: whether or not to provide the encoder output to the decoder\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None, # Attention mask for encoder output (if used)\n",
    "        labels: Optional[torch.LongTensor] = None, # Target tokens for next-token prediction during training\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None, # Whether to return attention weights (for interpretability/debugging)\n",
    "        output_hidden_states: Optional[bool] = None, # Whether or not to return the hidden states of all layers. (for interpretability/debugging)\n",
    "        return_dict: Optional[bool] = None, # Whether to return a `ModelOutput` object instead of a tuple\n",
    "        **kwargs,  # Additional arguments for compatibility with HuggingFace Trainer\n",
    "    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]: # specifies the type of return\n",
    "\n",
    "        # Handle custom input format: if input_ids is an Interaction object, extract its fields\n",
    "        # Hopwise uses the Interaction class to manage and represent input data in a structured way.\n",
    "        if isinstance(input_ids, Interaction):\n",
    "            token_type_ids = input_ids[\"token_type_ids\"]   # Type information: ENTITY, RELATION, SPECIAL\n",
    "            attention_mask = input_ids[\"attention_mask\"]   # Indicates valid tokens (non-PAD)\n",
    "            input_ids = input_ids[\"input_ids\"]             # Actual token indices\n",
    "\n",
    "        # Inference mode: dynamically align token_type_ids with input_ids length\n",
    "        # During generation (inference), input_ids grows one token at a time.\n",
    "        # Since self.token_type_ids is a fixed-length template, we slice it to match the current input length.\n",
    "        # Example: if input_ids has length 4, we select the first 3 token types from the template and repeat the last one (usually <SPECIAL>) as fallback.\n",
    "        # This ensures the token_type_ids has the same length as input_ids, preventing shape mismatches.\n",
    "        # We then expand the token_type_ids across the batch dimension.\n",
    "        if self.use_kg_token_types:\n",
    "            token_type_ids = self.token_type_ids[[*range(input_ids.shape[1] - 1), -1]]\n",
    "            token_type_ids = token_type_ids.expand(input_ids.shape[0], -1)\n",
    "\n",
    "        # Pass inputs to the GPT2 transformer\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache and labels is None,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict or self.config.use_return_dict,\n",
    "        )\n",
    "\n",
    "        # Extract the last hidden states (one vector per token)\n",
    "        sequence_output = transformer_outputs[0]\n",
    "\n",
    "        # Apply the language modeling head to generate prediction scores (logits) for each token\n",
    "        prediction_scores = self.lm_head(sequence_output)\n",
    "\n",
    "        # Prepare to compute training loss if labels are provided\n",
    "        lm_loss = None\n",
    "        if labels is not None:\n",
    "\n",
    "            # Shift logits and labels for next-token prediction (autoregressive)\n",
    "            # Shift prediction scores and input ids by one\n",
    "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            # Compute cross-entropy loss between predicted logits and actual next tokens\n",
    "            lm_loss = self.loss(\n",
    "                shifted_prediction_scores.view(-1, self.config.vocab_size),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "\n",
    "        # Handle output formatting (tuple or dictionary) based on return_dict flag\n",
    "\n",
    "        # --- return_dict=False ----\n",
    "        # Return a dictionary\n",
    "        # training-mode: (loss, logits, hidden_states, attentions)\n",
    "        # inference-mode: (logits, hidden_states, attentions)\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + transformer_outputs[2:]\n",
    "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
    "\n",
    "        # --- return_dict=True ----\n",
    "        # Return standard HuggingFace output object\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c02b3a",
   "metadata": {
    "id": "66c02b3a"
   },
   "source": [
    "#### Method Overriding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e3aee",
   "metadata": {
    "id": "fa1e3aee"
   },
   "source": [
    "PEARLM extends the GPT2LMHeadModel class.\n",
    "\n",
    "The `forward` method is overrided to include custom logic for handling type embeddings (`ENTITY`, `RELATION`, `SPECIAL`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d34c9f",
   "metadata": {
    "id": "19d34c9f"
   },
   "outputs": [],
   "source": [
    "def forward(...Args...) -> Union[tuple, CausalLMOutputWithCrossAttentions]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64b760",
   "metadata": {
    "id": "6f64b760"
   },
   "source": [
    "Args:\n",
    "\n",
    "| Parameter               | Description |\n",
    "|-------------------------|-------------|\n",
    "| `input_ids`             | Indices of input sequence tokens in the vocabulary. |\n",
    "| `past_key_values`       | Cached (key, value) pairs to speed up autoregressive decoding. |\n",
    "| `attention_mask`        | Prevents the model from giving weight to padding tokens (PAD). |\n",
    "| `token_type_ids`        | Segment token indices to indicate first and second portions of the inputs. |\n",
    "| `position_ids`          | Positional indices for each token. |\n",
    "| `head_mask`             | Disables specific attention heads in the model. |\n",
    "| `inputs_embeds`         | Embedded representation of the `input_ids`. |\n",
    "| `encoder_hidden_states` | Used in encoder-decoder settings to provide the encoder output to the decoder. |\n",
    "| `encoder_attention_mask`| Attention mask for the encoder output (if used). |\n",
    "| `labels`                | Target tokens for next-token prediction during training. |\n",
    "| `use_cache`             | Whether to use caching to speed up generation. |\n",
    "| `output_attentions`     | Whether to return attention weights (useful for interpretability/debugging). |\n",
    "| `output_hidden_states`  | Whether to return hidden states of all layers (for interpretability/debugging). |\n",
    "| `return_dict`           | Whether to return a `ModelOutput` object instead of a tuple. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa92762",
   "metadata": {
    "id": "ffa92762"
   },
   "source": [
    "#### Output Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfa531",
   "metadata": {
    "id": "82dfa531"
   },
   "outputs": [],
   "source": [
    "def forward(\n",
    "\n",
    "    #--Args --#\n",
    "\n",
    "    return_dict: Optional[bool] = None,\n",
    "\n",
    ") -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n",
    "\n",
    "    #-- Other Code --#\n",
    "\n",
    "    # Handle output formatting (tuple or dictionary) based on return_dict flag\n",
    "\n",
    "    # --- return_dict=True ----\n",
    "    # Return a dictionary\n",
    "    # training-mode: (loss, logits, hidden_states, attentions)\n",
    "    # inference-mode: (logits, hidden_states, attentions)\n",
    "    if not return_dict:\n",
    "        output = (prediction_scores,) + transformer_outputs[2:]\n",
    "        return ((lm_loss,) + output) if lm_loss is not None else output\n",
    "\n",
    "    # --- return_dict=False ----\n",
    "    # Return standard HuggingFace output object\n",
    "    return CausalLMOutputWithCrossAttentions(\n",
    "        loss=lm_loss,\n",
    "        logits=prediction_scores,\n",
    "        past_key_values=transformer_outputs.past_key_values,\n",
    "        hidden_states=transformer_outputs.hidden_states,\n",
    "        attentions=transformer_outputs.attentions,\n",
    "        cross_attentions=transformer_outputs.cross_attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a05f1b",
   "metadata": {
    "id": "23a05f1b"
   },
   "source": [
    "#### Pass inputs to the Casual Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uS20dQGMdq_c",
   "metadata": {
    "id": "uS20dQGMdq_c"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/pearlm-pass-input.png\" alt=\"PEARLM Pass Input\" width=\"450\" height=\"250\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc7c3fd",
   "metadata": {
    "id": "bbc7c3fd"
   },
   "outputs": [],
   "source": [
    "# Pass inputs to the GPT2 transformer\n",
    "transformer_outputs = self.transformer(\n",
    "    input_ids,\n",
    "    past_key_values=past_key_values,\n",
    "    attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    "    position_ids=position_ids,\n",
    "    head_mask=head_mask,\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    "    use_cache=use_cache and labels is None,\n",
    "    output_attentions=output_attentions,\n",
    "    output_hidden_states=output_hidden_states,\n",
    "    return_dict=return_dict or self.config.use_return_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b4b35",
   "metadata": {
    "id": "a19b4b35"
   },
   "source": [
    "#### Prediction score generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ATYWWatyd59q",
   "metadata": {
    "id": "ATYWWatyd59q"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/pearlm-prediction-score-generation.png\" alt=\"PEARLM Prediction Score Generation\" width=\"450\" height=\"250\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62552efd",
   "metadata": {
    "id": "62552efd"
   },
   "outputs": [],
   "source": [
    "# Extract the last hidden states (one vector per token)\n",
    "sequence_output = transformer_outputs[0]\n",
    "\n",
    "# Apply the language modeling head to generate prediction scores (logits) for each token\n",
    "prediction_scores = self.lm_head(sequence_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be35820",
   "metadata": {
    "id": "5be35820"
   },
   "source": [
    "#### Training phase: Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717b3d2",
   "metadata": {
    "id": "4717b3d2"
   },
   "outputs": [],
   "source": [
    "# Prepare to compute training loss if labels are provided\n",
    "lm_loss = None\n",
    "if labels is not None:\n",
    "\n",
    "    # Shift logits and labels for next-token prediction (autoregressive)\n",
    "    # Shift prediction scores and input ids by one\n",
    "    shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()  # Remove last token (usually EOS)\n",
    "    labels = labels[:, 1:].contiguous()  # Remove first token (usually BOS)\n",
    "\n",
    "    # Compute cross-entropy loss between predicted logits and actual next tokens\n",
    "    lm_loss = self.loss(\n",
    "        shifted_prediction_scores.view(-1, self.config.vocab_size),\n",
    "        labels.view(-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a33c8",
   "metadata": {
    "id": "d79a33c8"
   },
   "source": [
    "Imagine to have the following sequence path:\n",
    "\n",
    "```python\n",
    "input_ids = [\"BOS\", \"Brad_Pitt\", \"acted_in\", \"Fight_Club\", \"EOS\"]\n",
    "```\n",
    "\n",
    "Each token at position `i` must predict the token at position `i+1`.\n",
    "\n",
    "| Token in Input | Should Predict |\n",
    "|----------------|----------------|\n",
    "| BOS            | Brad Pitt      |\n",
    "| Brad Pitt      | acted_in       |\n",
    "| acted_in       | Fight_Club     |\n",
    "| Fight_Club     | EOS            |\n",
    "\n",
    "So the correct `labels` (i.e., the tokens to be predicted) are:\n",
    "```python\n",
    "labels    = [\"Brad_Pitt\", \"acted_in\", \"Fight_Club\", \"EOS\"]  # without EOS\n",
    "```\n",
    "\n",
    "\n",
    "We don't need the `[BOS]`\n",
    "token between the labels \u2192 so we shift right (skip the first token):\n",
    "```python\n",
    "labels = labels[:, 1:].contiguous()  # Starting with the token in position 1 and moving to the right\n",
    "```\n",
    "\n",
    "Equally, after `[EOS]` there is nothing to predict. So we can remove it from the tokens\n",
    "```python\n",
    "shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239d3be",
   "metadata": {
    "id": "0239d3be"
   },
   "source": [
    "---\n",
    "<a id=\"#S3\"></a>\n",
    "## \ud83d\udd2e 3. Prediction Interface (`predict`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb045707",
   "metadata": {
    "id": "cb045707"
   },
   "source": [
    "- A convenience method for inference that internally calls `forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805093a7",
   "metadata": {
    "id": "805093a7"
   },
   "outputs": [],
   "source": [
    "# method to do inference\n",
    "def predict(self, input_ids, **kwargs):\n",
    "    return self.forward(input_ids, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70215a",
   "metadata": {
    "id": "3a70215a"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c1aaf",
   "metadata": {},
   "source": [
    "## \u2728 4. Generate Interface (`generate`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0a532",
   "metadata": {},
   "source": [
    "The `generate` method is responsible for producing sequences of tokens based on the input provided. It overrides the HuggingFace `GPT2LMHeadModel`'s `generate` method by incorporating custom logic specific to the PEARLM model.\n",
    "\n",
    "Key features of the `generate` method:\n",
    "- **Logits Processor**: Applies a list of custom constraints (`logits_processor_list`) to the model's logits during generation. This ensures that the generated tokens adhere to specific rules, such as knowledge graph constraints. In PEARLM, the default logits processor is KGCD, implemented by the class `ConstrainedLogitsProcessorWordLevel`\n",
    "- **Paths per User**: Specifies the number of sequences to generate for each user during recommendation tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, inputs, **kwargs):\n",
    "    kwargs[\"logits_processor\"] = self.logits_processor_list\n",
    "    kwargs[\"num_return_sequences\"] = kwargs.pop(\"paths_per_user\")\n",
    "    return super(GPT2LMHeadModel, self).generate(**inputs, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288f3c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb378b",
   "metadata": {
    "id": "f3bb378b"
   },
   "source": [
    "<a id=\"#S5\"></a>\n",
    "\n",
    "# 2\ufe0f\u20e3 Decoding Stage: Knowledge Graph-Constrained Decoding (KGCD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b062c",
   "metadata": {
    "id": "257b062c"
   },
   "source": [
    "Decoding in causal language models (CLM), such as GPT-2, involves generating a sequence of tokens sequentially. In PEARLM,\n",
    "this process is enhanced with the Knowledge Graph-Constrained Decoding (KGCD) method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02852fb5",
   "metadata": {
    "id": "02852fb5"
   },
   "source": [
    "<div style=\"background-color: #fff8e1; border-left: 5px solid #ffc107; padding: 16px 20px; margin: 15px 0; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.05); font-family: sans-serif; line-height: 1.6;\">\n",
    "  <p style=\"margin: 0;\">\n",
    "    <b>\ud83d\udca1</b> The objective of the <b>Knowledge Graph-Constrained Decoding </b> in PEARLM is to ensure that the generated explanation paths strictly adhere to the actual structure of the Knowledge Graph.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556082f",
   "metadata": {
    "id": "f556082f"
   },
   "source": [
    "Specifically, KGCD works by modifying the decoding step in causal language models so that only valid next tokens (i.e., entities or relations that are logically and structurally reachable according to the KG) are considered during generation. If a token is not valid in the KG context, its probability is set to negative infinity, making it not selectable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pb6fQbaqeIEi",
   "metadata": {
    "id": "pb6fQbaqeIEi"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/pearlm-kgcd-2.png\" alt=\"PEARLM KGCD 2\" width=\"600\" height=\"250\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab395b63",
   "metadata": {
    "id": "ab395b63"
   },
   "source": [
    "The KGCD is implemented by the class [ConstrainedLogitsProcessorWordLevel](https://github.com/tail-unica/hopwise/blob/main/hopwise/model/logits_processor.py) declared in `hopwise/hopwise/model/logits_processor.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57178536",
   "metadata": {
    "id": "57178536"
   },
   "outputs": [],
   "source": [
    "class ConstrainedLogitsProcessorWordLevel(LogitsProcessor):\n",
    "    # Extends HuggingFace's LogitsProcessor to implement KG-Constrained Decoding (KGCD),\n",
    "    # which filters the logits based on the structure of the Knowledge Graph (KG)\n",
    "\n",
    "        # ...........\n",
    "\n",
    "\n",
    "        #######\n",
    "        ## Some auxiliary functions\n",
    "        ######\n",
    "\n",
    "\n",
    "        # ...........\n",
    "\n",
    "        # Main method\n",
    "        def __call__(self, input_ids, scores):\n",
    "        # input_ids: partial sequences generated so far.\n",
    "        # scores: model logits for the next token.\n",
    "\n",
    "        current_len = input_ids.shape[-1]  # current length of the sequence\n",
    "        has_bos_token = self.is_bos_token_in_input(input_ids)  # True if the first token in the sequence is <bos>\n",
    "\n",
    "        # generation step \u2192 apply KG-based constraints to the logits\n",
    "\n",
    "        # Initially, consider all sequences generated so far (e.g., from beam search)\n",
    "        # In the following steps (for recommendation tasks), only sequences with unique end-contexts will be selected\n",
    "\n",
    "        ## Example: input_ids might contain 5 partial sequences like\n",
    "        # [ [user123, watched], [user123, watched], [user123, liked], [user456, watched], [user123, watched] ]\n",
    "        # In this case, the first, second, and last are identical in their last tokens\n",
    "        unique_input_ids = input_ids\n",
    "\n",
    "        # If we are in the recommendation task and not yet at the last generation step,\n",
    "        # apply deduplication to optimize the computation of KG constraints\n",
    "        if self.task == self.RECOMMENDATION_TASK and current_len < self.max_sequence_length - 1 - has_bos_token:\n",
    "\n",
    "            # Determine whether the next token to generate is a relation or an entity:\n",
    "            # - If the next token is a relation \u2192 only the last entity is needed (1 token) \u2192 (e.g., [user123]) \u2192 last_n_tokens = 1\n",
    "            # - If the next token is an entity \u2192 the last 2 tokens are needed (entity, relation) \u2192 (e.g., [user123, watched]) \u2192 last_n_tokens = 2\n",
    "            last_n_tokens = 2 if self.is_next_token_entity(input_ids) else 1\n",
    "\n",
    "            # Apply deduplication: select unique sequences based only on the relevant last tokens (1 or 2)\n",
    "            # This avoids recomputing the same mask for sequences that share the same context\n",
    "\n",
    "            # np.unique returns:\n",
    "            # - input_ids_indices \u2192 positions of unique sequences\n",
    "            # - input_ids_inv \u2192 for each original row, indicates which unique row it corresponds to\n",
    "            #\n",
    "            # Example (with last_n_tokens=2):\n",
    "            #   input_ids[:, -2:] \u2192 [ [user123, watched], [user123, watched], [user123, liked], [user456, watched], [user123, watched] ]\n",
    "            #   np.unique(...) \u2192 keeps only 3 unique ones: [ [user123, watched], [user123, liked], [user456, watched] ]\n",
    "            #   input_ids_indices = [0, 2, 3]\n",
    "            #   input_ids_inv = [0, 0, 1, 2, 0] \u2192 maps each original row to its corresponding unique row\n",
    "\n",
    "            _, input_ids_indices, input_ids_inv = np.unique(\n",
    "                input_ids.cpu().numpy()[:, -last_n_tokens:],  # relevant tokens for KG constraint\n",
    "                axis=0,\n",
    "                return_index=True,   # positions of unique sequences\n",
    "                return_inverse=True  # maps each original row to its deduplicated sequence\n",
    "            )\n",
    "\n",
    "            # Update the sequences for which masks will be computed: only those with unique contexts\n",
    "            # Example: unique_input_ids = input_ids[[0, 2, 3]]\n",
    "            unique_input_ids = input_ids[input_ids_indices]\n",
    "\n",
    "        # Initialize the token mask matrix: shape [#unique_sequences, vocab_size]\n",
    "        full_mask = np.zeros((unique_input_ids.shape[0], len(self.tokenizer)), dtype=bool)\n",
    "\n",
    "\n",
    "        # For each sequence, identify valid and invalid tokens for the next step\n",
    "        # invalid tokens = True, valid tokens = False\n",
    "        #\n",
    "        # Example unique_input_ids:\n",
    "        # [\n",
    "        #   [user123, watched],     # idx = 0\n",
    "        #   [user123, liked],       # idx = 1\n",
    "        #   [user456, watched]      # idx = 2\n",
    "        # ]\n",
    "        # For each sequence, we query the KG to get the valid tokens to generate next\n",
    "        for idx in range(unique_input_ids.shape[0]):\n",
    "            if self.task == self.RECOMMENDATION_TASK:\n",
    "                # Extracts the \"key\" from the context (last 1 or 2 tokens, e.g., [user123, watched])\n",
    "                # Queries the KG to find which tokens are valid as next steps\n",
    "                # Example:\n",
    "                #   key = (user123, watched)\n",
    "                #   candidate_tokens = [movieA, movieB]\n",
    "                key, candidate_tokens = self.process_scores_rec(unique_input_ids, idx)\n",
    "\n",
    "            elif self.task == self.LINK_PREDICTION_TASK:\n",
    "                # Alternative case for link prediction (Under Development)\n",
    "                key, candidate_tokens = self.process_scores_lp(unique_input_ids, idx)\n",
    "\n",
    "\n",
    "            # Creates a boolean mask as long as the vocabulary:\n",
    "            # - tokens NOT in candidate_tokens are set to True (invalid)\n",
    "            # - tokens in candidate_tokens are set to False (valid)\n",
    "            #\n",
    "            # Example:\n",
    "            #   vocab = [watched, liked, movieA, movieB, movieC]\n",
    "            #   candidate_tokens = [movieA, movieB]\n",
    "            #   banned_mask = [True, True, False, False, True]\n",
    "            banned_mask = self.get_banned_mask(key, candidate_tokens)\n",
    "\n",
    "            # Stores the mask in the corresponding row of full_mask\n",
    "            # full_mask becomes a [unique_sequences x vocab] matrix with True for disallowed tokens\n",
    "            full_mask[idx] = banned_mask\n",
    "\n",
    "\n",
    "        # Applies the mask to the logits, setting -inf for the banned tokens:\n",
    "        # This prevents the model from generating invalid tokens according to the KG\n",
    "        # - In the recommendation task, since deduplication occurred, we remap the mask to each original sequence\n",
    "        # - Otherwise, we apply it directly\n",
    "\n",
    "\n",
    "        # In this case we need to \"remap\" each original sequence\n",
    "        # to its corresponding deduplicated mask (using input_ids_inv)\n",
    "        #\n",
    "        # Example:\n",
    "        # - input_ids has 5 original sequences\n",
    "        # - unique_input_ids has 3 unique contexts\n",
    "        # - input_ids_inv = [0, 0, 1, 2, 0] \u2192 maps each original row to its row in full_mask\n",
    "        # - full_mask = [\n",
    "        #     [True, True, False, False, True],   # context (user123, watched)\n",
    "        #     [True, True, True, True, False],    # context (user123, liked)\n",
    "        #     [True, True, False, True, True]     # context (user456, watched)\n",
    "        #   ]\n",
    "        # \u2192 scores[full_mask[input_ids_inv]] = -inf \u2192 mask applied to each original sequence\n",
    "        if self.task == self.RECOMMENDATION_TASK and current_len < self.max_sequence_length - 1 - has_bos_token:\n",
    "            scores[full_mask[input_ids_inv]] = -math.inf\n",
    "        else:\n",
    "            scores[full_mask] = -math.inf\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7620",
   "metadata": {
    "id": "987c7620"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569a93c",
   "metadata": {
    "id": "2569a93c"
   },
   "source": [
    "<a name=\"r1\">[1]</a> Balloccu, G., Boratto, L., Cancedda, C., Fenu, G., & Marras, M. (2023). Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. arXiv preprint arXiv:2310.16452. https://arxiv.org/abs/2310.16452"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1afe4",
   "metadata": {
    "id": "94d1afe4"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}